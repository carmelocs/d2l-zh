{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归的简洁实现\n",
    "\n",
    "随着深度学习框架的发展，开发深度学习应用变得越来越便利。实践中，我们通常可以用比上一节更简洁的代码来实现同样的模型。在本节中，我们将介绍如何使用MXNet提供的Gluon接口更方便地实现线性回归的训练。\n",
    "\n",
    "## 生成数据集\n",
    "\n",
    "我们生成与上一节中相同的数据集。其中`features`是训练数据特征，`labels`是标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd, nd\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = nd.random.normal(scale=1, shape=(num_examples, num_inputs))\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += nd.random.normal(scale=0.01, shape=labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "Gluon提供了`data`包来读取数据。由于`data`常用作变量名，我们将导入的`data`模块用添加了Gluon首字母的假名`gdata`代替。在每一次迭代中，我们将随机读取包含10个数据样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import data as gdata\n",
    "\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = gdata.ArrayDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = gdata.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里`data_iter`的使用与上一节中的一样。让我们读取并打印第一个小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 2.3101764   0.24071898]\n",
      " [-0.662727    1.8137852 ]\n",
      " [-0.17675017 -1.7476647 ]\n",
      " [-0.572525   -0.80434114]\n",
      " [-0.8103949   2.1938179 ]\n",
      " [-0.9286441   2.0619104 ]\n",
      " [-1.2878888   1.5034332 ]\n",
      " [ 1.4984084  -0.45594302]\n",
      " [-0.29596215  1.8380349 ]\n",
      " [ 0.9561315  -0.8629183 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 8.0141945 -3.2972414  9.787475   5.782796  -4.863949  -4.662165\n",
      " -3.4834092  8.734471  -2.645789   9.049869 ]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "在上一节从零开始的实现中，我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更烦琐。其实，Gluon提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用Gluon更简洁地定义线性回归。\n",
    "\n",
    "首先，导入`nn`模块。实际上，“nn”是neural networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。我们先定义一个模型变量`net`，它是一个`Sequential`实例。在Gluon中，`Sequential`实例可以看作是一个串联各个层的容器。在构造模型时，我们在该容器中依次添加层。当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾图3.1中线性回归在神经网络图中的表示。作为一个单层神经网络，线性回归输出层中的神经元和输入层中各个输入完全连接。因此，线性回归的输出层又叫全连接层。在Gluon中，全连接层是一个`Dense`实例。我们定义该层输出个数为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "net.add(nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，在Gluon中我们无须指定每一层输入的形状，例如线性回归的输入个数。当模型得到数据时，例如后面执行`net(X)`时，模型将自动推断出每一层的输入个数。我们将在之后“深度学习计算”一章详细介绍这种机制。Gluon的这一设计为模型开发带来便利。\n",
    "\n",
    "\n",
    "## 初始化模型参数\n",
    "\n",
    "在使用`net`前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。我们从MXNet导入`init`模块。该模块提供了模型参数初始化的各种方法。这里的`init`是`initializer`的缩写形式。我们通过`init.Normal(sigma=0.01)`指定权重参数每个元素将在初始化时随机采样于均值为0、标准差为0.01的正态分布。偏差参数默认会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import init\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.9995346 -3.3999894]]\n",
       "<NDArray 1x2 @cpu(0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "在Gluon中，`loss`模块定义了各种损失函数。我们用假名`gloss`代替导入的`loss`模块，并直接使用它提供的平方损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import loss as gloss\n",
    "\n",
    "loss = gloss.L2Loss()  # 平方损失又称L2范数损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class L2Loss in module mxnet.gluon.loss:\n",
      "\n",
      "class L2Loss(Loss)\n",
      " |  L2Loss(weight=1.0, batch_axis=0, **kwargs)\n",
      " |  \n",
      " |  Calculates the mean squared error between `label` and `pred`.\n",
      " |  \n",
      " |  .. math:: L = \\frac{1}{2} \\sum_i \\vert {label}_i - {pred}_i \\vert^2.\n",
      " |  \n",
      " |  `label` and `pred` can have arbitrary shape as long as they have the same\n",
      " |  number of elements.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  weight : float or None\n",
      " |      Global scalar weight for loss.\n",
      " |  batch_axis : int, default 0\n",
      " |      The axis that represents mini-batch.\n",
      " |  \n",
      " |  \n",
      " |  Inputs:\n",
      " |      - **pred**: prediction tensor with arbitrary shape\n",
      " |      - **label**: target tensor with the same size as pred.\n",
      " |      - **sample_weight**: element-wise weighting tensor. Must be broadcastable\n",
      " |        to the same shape as pred. For example, if pred has shape (64, 10)\n",
      " |        and you want to weigh each sample in the batch separately,\n",
      " |        sample_weight should have shape (64, 1).\n",
      " |  \n",
      " |  Outputs:\n",
      " |      - **loss**: loss tensor with shape (batch_size,). Dimenions other than\n",
      " |        batch_axis are averaged out.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      L2Loss\n",
      " |      Loss\n",
      " |      mxnet.gluon.block.HybridBlock\n",
      " |      mxnet.gluon.block.Block\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, weight=1.0, batch_axis=0, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  hybrid_forward(self, F, pred, label, sample_weight=None)\n",
      " |      Overrides to construct symbolic graph for this `Block`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Symbol or NDArray\n",
      " |          The first input tensor.\n",
      " |      *args : list of Symbol or list of NDArray\n",
      " |          Additional input tensors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Loss:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.HybridBlock:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Registers parameters.\n",
      " |  \n",
      " |  cast(self, dtype)\n",
      " |      Cast this Block to use another data type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or numpy.dtype\n",
      " |          The new data type.\n",
      " |  \n",
      " |  export(self, path, epoch=0, remove_amp_cast=True)\n",
      " |      Export HybridBlock to json format that can be loaded by\n",
      " |      `gluon.SymbolBlock.imports`, `mxnet.mod.Module` or the C++ interface.\n",
      " |      \n",
      " |      .. note:: When there are only one input, it will have name `data`. When there\n",
      " |                Are more than one inputs, they will be named as `data0`, `data1`, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Path to save model. Two files `path-symbol.json` and `path-xxxx.params`\n",
      " |          will be created, where xxxx is the 4 digits epoch number.\n",
      " |      epoch : int\n",
      " |          Epoch number of saved model.\n",
      " |  \n",
      " |  forward(self, x, *args)\n",
      " |      Defines the forward computation. Arguments can be either\n",
      " |      :py:class:`NDArray` or :py:class:`Symbol`.\n",
      " |  \n",
      " |  hybridize(self, active=True, backend=None, backend_opts=None, **kwargs)\n",
      " |      Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n",
      " |      non-hybrid children.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      active : bool, default True\n",
      " |          Whether to turn hybrid on or off.\n",
      " |      backend : str\n",
      " |          The name of backend, as registered in `SubgraphBackendRegistry`, default None\n",
      " |      backend_opts : dict of user-specified options to pass to the backend for partitioning, optional\n",
      " |          Passed on to `PrePartition` and `PostPartition` functions of `SubgraphProperty`\n",
      " |      static_alloc : bool, default False\n",
      " |          Statically allocate memory to improve speed. Memory usage may increase.\n",
      " |      static_shape : bool, default False\n",
      " |          Optimize for invariant input shapes between iterations. Must also\n",
      " |          set static_alloc to True. Change of input shapes is still allowed\n",
      " |          but slower.\n",
      " |  \n",
      " |  infer_shape(self, *args)\n",
      " |      Infers shape of Parameters from inputs.\n",
      " |  \n",
      " |  infer_type(self, *args)\n",
      " |      Infers data type of Parameters from inputs.\n",
      " |  \n",
      " |  optimize_for(self, x, *args, backend=None, backend_opts=None, **kwargs)\n",
      " |      Partitions the current HybridBlock and optimizes it for a given backend\n",
      " |      without executing a forward pass. Modifies the HybridBlock in-place.\n",
      " |      \n",
      " |      Immediately partitions a HybridBlock using the specified backend. Combines\n",
      " |      the work done in the hybridize API with part of the work done in the forward\n",
      " |      pass without calling the CachedOp. Can be used in place of hybridize,\n",
      " |      afterwards `export` can be called or inference can be run. See README.md in\n",
      " |      example/extensions/lib_subgraph/README.md for more details.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      # partition and then export to file\n",
      " |      block.optimize_for(x, backend='myPart')\n",
      " |      block.export('partitioned')\n",
      " |      \n",
      " |      # partition and then run inference\n",
      " |      block.optimize_for(x, backend='myPart')\n",
      " |      block(x)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : NDArray\n",
      " |          first input to model\n",
      " |      *args : NDArray\n",
      " |          other inputs to model\n",
      " |      backend : str\n",
      " |          The name of backend, as registered in `SubgraphBackendRegistry`, default None\n",
      " |      backend_opts : dict of user-specified options to pass to the backend for partitioning, optional\n",
      " |          Passed on to `PrePartition` and `PostPartition` functions of `SubgraphProperty`\n",
      " |      static_alloc : bool, default False\n",
      " |          Statically allocate memory to improve speed. Memory usage may increase.\n",
      " |      static_shape : bool, default False\n",
      " |          Optimize for invariant input shapes between iterations. Must also\n",
      " |          set static_alloc to True. Change of input shapes is still allowed\n",
      " |          but slower.\n",
      " |  \n",
      " |  register_child(self, block, name=None)\n",
      " |      Registers block as a child of self. :py:class:`Block` s assigned to self as\n",
      " |      attributes will be registered automatically.\n",
      " |  \n",
      " |  register_op_hook(self, callback, monitor_all=False)\n",
      " |      Install op hook for block recursively.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      callback : function\n",
      " |          Takes a string and a NDArrayHandle.\n",
      " |      monitor_all : bool, default False\n",
      " |          If true, monitor both input and output, otherwise monitor output only.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __call__(self, *args)\n",
      " |      Calls forward. Only accepts positional arguments.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every child block as well as self.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fn : callable\n",
      " |          Function to be applied to each submodule, of form `fn(block)`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      this block\n",
      " |  \n",
      " |  collect_params(self, select=None)\n",
      " |      Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n",
      " |      children's Parameters(default), also can returns the select :py:class:`ParameterDict`\n",
      " |      which match some given regular expressions.\n",
      " |      \n",
      " |      For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',\n",
      " |      'fc_bias']::\n",
      " |      \n",
      " |          model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')\n",
      " |      \n",
      " |      or collect all parameters whose names end with 'weight' or 'bias', this can be done\n",
      " |      using regular expressions::\n",
      " |      \n",
      " |          model.collect_params('.*weight|.*bias')\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      select : str\n",
      " |          regular expressions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The selected :py:class:`ParameterDict`\n",
      " |  \n",
      " |  initialize(self, init=<mxnet.initializer.Uniform object at 0x7fceca174820>, ctx=None, verbose=False, force_reinit=False)\n",
      " |      Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n",
      " |      Equivalent to ``block.collect_params().initialize(...)``\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      init : Initializer\n",
      " |          Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n",
      " |          Otherwise, :py:meth:`Parameter.init` takes precedence.\n",
      " |      ctx : Context or list of Context\n",
      " |          Keeps a copy of Parameters on one or many context(s).\n",
      " |      verbose : bool, default False\n",
      " |          Whether to verbosely print out details on initialization.\n",
      " |      force_reinit : bool, default False\n",
      " |          Whether to force re-initialization if parameter is already initialized.\n",
      " |  \n",
      " |  load_parameters(self, filename, ctx=None, allow_missing=False, ignore_extra=False, cast_dtype=False, dtype_source='current')\n",
      " |      Load parameters from file previously saved by `save_parameters`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |      cast_dtype : bool, default False\n",
      " |          Cast the data type of the NDArray loaded from the checkpoint to the dtype\n",
      " |          provided by the Parameter if any.\n",
      " |      dtype_source : str, default 'current'\n",
      " |          must be in {'current', 'saved'}\n",
      " |          Only valid if cast_dtype=True, specify the source of the dtype for casting\n",
      " |          the parameters\n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html>`_\n",
      " |  \n",
      " |  load_params(self, filename, ctx=None, allow_missing=False, ignore_extra=False)\n",
      " |      [Deprecated] Please use load_parameters.\n",
      " |      \n",
      " |      Load parameters from file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to parameter file.\n",
      " |      ctx : Context or list of Context, default cpu()\n",
      " |          Context(s) to initialize loaded parameters on.\n",
      " |      allow_missing : bool, default False\n",
      " |          Whether to silently skip loading parameters not represents in the file.\n",
      " |      ignore_extra : bool, default False\n",
      " |          Whether to silently ignore parameters from the file that are not\n",
      " |          present in this Block.\n",
      " |  \n",
      " |  name_scope(self)\n",
      " |      Returns a name space object managing a child :py:class:`Block` and parameter\n",
      " |      names. Should be used within a ``with`` statement::\n",
      " |      \n",
      " |          with self.name_scope():\n",
      " |              self.dense = nn.Dense(20)\n",
      " |      \n",
      " |      Please refer to\n",
      " |      `the naming tutorial </api/python/docs/tutorials/packages/gluon/blocks/naming.html>`_\n",
      " |      for more info on prefix and naming.\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately after :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input, output) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the block.\n",
      " |      \n",
      " |      The hook function is called immediately before :func:`forward`.\n",
      " |      It should not modify the input or output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      hook : callable\n",
      " |          The forward hook function of form `hook(block, input) -> None`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`mxnet.gluon.utils.HookHandle`\n",
      " |  \n",
      " |  save_parameters(self, filename, deduplicate=False)\n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      Saved parameters can only be loaded with `load_parameters`. Note that this\n",
      " |      method only saves parameters, not model structure. If you want to save\n",
      " |      model structures, please use :py:meth:`HybridBlock.export`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |      deduplicate : bool, default False\n",
      " |          If True, save shared parameters only once. Otherwise, if a Block\n",
      " |          contains multiple sub-blocks that share parameters, each of the\n",
      " |          shared parameters will be separately saved for every sub-block.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      `Saving and Loading Gluon Models         <https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html>`_\n",
      " |  \n",
      " |  save_params(self, filename)\n",
      " |      [Deprecated] Please use save_parameters. Note that if you want load\n",
      " |      from SymbolBlock later, please use export instead.\n",
      " |      \n",
      " |      Save parameters to file.\n",
      " |      \n",
      " |      filename : str\n",
      " |          Path to file.\n",
      " |  \n",
      " |  summary(self, *inputs)\n",
      " |      Print the summary of the model's output and parameters.\n",
      " |      \n",
      " |      The network must have been initialized, and must not have been hybridized.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputs : object\n",
      " |          Any input that the model supports. For any tensor in the input, only\n",
      " |          :class:`mxnet.ndarray.NDArray` is supported.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  name\n",
      " |      Name of this :py:class:`Block`, without '_' in the end.\n",
      " |  \n",
      " |  params\n",
      " |      Returns this :py:class:`Block`'s parameter dictionary (does not include its\n",
      " |      children's parameters).\n",
      " |  \n",
      " |  prefix\n",
      " |      Prefix of this :py:class:`Block`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from mxnet.gluon.block.Block:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gloss.L2Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化算法\n",
    "\n",
    "同样，我们也无须实现小批量随机梯度下降。在导入Gluon后，我们创建一个`Trainer`实例，并指定学习率为0.03的小批量随机梯度下降（`sgd`）为优化算法。该优化算法将用来迭代`net`实例所有通过`add`函数嵌套的层所包含的全部参数。这些参数可以通过`collect_params`函数获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "在使用Gluon训练模型时，我们通过调用`Trainer`实例的`step`函数来迭代模型参数。上一节中我们提到，由于变量`l`是长度为`batch_size`的一维`NDArray`，执行`l.backward()`等价于执行`l.sum().backward()`。按照小批量随机梯度下降的定义，我们在`step`函数中指明批量大小，从而对批量中样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000050\n",
      "epoch 2, loss: 0.000050\n",
      "epoch 3, loss: 0.000050\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            l = loss(net(X), y)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "    l = loss(net(features), labels)\n",
    "    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们分别比较学到的模型参数和真实的模型参数。我们从`net`获得需要的层，并访问其权重（`weight`）和偏差（`bias`）。学到的参数和真实的参数很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4],\n",
       " \n",
       " [[ 2.000062  -3.3997772]]\n",
       " <NDArray 1x2 @cpu(0)>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "true_w, dense.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2,\n",
       " \n",
       " [4.1999917]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias.data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 使用Gluon可以更简洁地实现模型。\n",
    "* 在Gluon中，`data`模块提供了有关数据处理的工具，`nn`模块定义了大量神经网络的层，`loss`模块定义了各种损失函数。\n",
    "* MXNet的`initializer`模块提供了模型参数初始化的各种方法。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 如果将`l = loss(net(X), y)`替换成`l = loss(net(X), y).mean()`，我们需要将`trainer.step(batch_size)`相应地改成`trainer.step(1)`。这是为什么呢？\n",
    "* 查阅MXNet文档，看看`gluon.loss`和`init`模块里提供了哪些损失函数和初始化方法。\n",
    "* 如何访问`dense.weight`的梯度？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/742)\n",
    "\n",
    "![](../img/qr_linear-regression-gluon.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l]",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
